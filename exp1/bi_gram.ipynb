{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, defaultdict\n",
    "from math import log2\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sentences = text.split(\"__eou__\")\n",
    "    sentences.pop()\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r\"[^\\w\\s']\", \"\", sentence).lower()\n",
    "        words += word_tokenize(sentence)\n",
    "        words.append(\"<beg>\")\n",
    "    return words\n",
    "\n",
    "\n",
    "def preprocess_text2(text):\n",
    "    sentences = text.split(\"__eou__\")\n",
    "    sentences.pop()\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        sentence = re.sub(r\"[^\\w\\s']\", \"\", sentence).lower()\n",
    "        words.append([\"<beg>\"] + word_tokenize(sentence) + [\"</end>\"])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(words):\n",
    "    vocab = Counter(words)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算bigram词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram(text):\n",
    "    bigram_counts = defaultdict(dict)\n",
    "    for sentence in text:\n",
    "        for i in range(len(sentence) - 1):\n",
    "            if sentence[i + 1] not in bigram_counts[sentence[i]]:\n",
    "                bigram_counts[sentence[i]][sentence[i + 1]] = 1\n",
    "            else:\n",
    "                bigram_counts[sentence[i]][sentence[i + 1]] += 1\n",
    "    return bigram_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算bigram概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bigram_probs(bigram_counts,vocab):\n",
    "    bigram_probs = defaultdict(dict)\n",
    "    for prev_word, list in bigram_counts.items():\n",
    "        for back_word, count in list.items():\n",
    "            bigram_probs[prev_word][back_word] = (count + 1) /(\n",
    "                vocab[prev_word] + len(vocab))\n",
    "    return bigram_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算句子困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_perplexity(text, bigram_probs, vocab, bigram_counts):\n",
    "    perplexity = []\n",
    "    for sentence in text:\n",
    "        prob = 0\n",
    "        for i in range(len(sentence) - 1):\n",
    "            if sentence[i] not in vocab:  # w1是未登录词\n",
    "                prob += len(vocab)\n",
    "            elif sentence[i + 1] not in bigram_probs[sentence[i]]: \n",
    "                # w1不是未登录词而w2是\n",
    "                prob += log2(1 / (vocab[sentence[i]] + len(vocab)))\n",
    "            else:\n",
    "                # 都不是未登录词\n",
    "                prob += log2(bigram_probs[sentence[i]][sentence[i + 1]])\n",
    "        perplexity.append(pow(2, -(prob / (len(sentence) - 1))))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675.7854405101131\n"
     ]
    }
   ],
   "source": [
    "def text_perplexity(perplexity):\n",
    "    return sum(perplexity) / len(perplexity)\n",
    "with open(\"train_LM.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "with open(\"test_LM.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    test_text = file.read()\n",
    "\n",
    "words = preprocess_text(text)\n",
    "vocab = build_vocab(words)\n",
    "\n",
    "train_text = preprocess_text2(text)\n",
    "bigram_counts = calculate_bigram(train_text)\n",
    "bigram_probs = calculate_bigram_probs(bigram_counts,vocab)\n",
    "test_text = preprocess_text2(test_text)\n",
    "perplexity = sentence_perplexity(test_text, bigram_probs, vocab, bigram_counts)\n",
    "test_perplexity = text_perplexity(perplexity)\n",
    "\n",
    "print(test_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
